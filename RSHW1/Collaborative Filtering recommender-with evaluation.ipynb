{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pojikC3bU7h",
        "outputId": "185de2fb-f515-4933-c382-fe4ecf15b655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTXLTIBEqXlq",
        "outputId": "11d1b28a-695d-4281-83dd-c96f363d89f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/My Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2**"
      ],
      "metadata": {
        "id": "wSCLQpBfv4cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "id": "Mz3oUeI2qmQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eeb0c13-d11e-4d74-e82e-a9ab437e92d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/772.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/772.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.4)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3162680 sha256=d28dab122dd9a5a3f6ed76a471070f8c494c0e09cdba86c09c6ecf9e080d912d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from surprise import KNNBasic\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from scipy.stats import pearsonr\n",
        "from collections import defaultdict\n",
        "import time"
      ],
      "metadata": {
        "id": "tfxTgu8RwKRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df = pd.read_csv('movielens_ratings.csv') #Read dataset\n",
        "ratings_df_600 = ratings_df.head(600000) #Get first 600000 rows of dataset"
      ],
      "metadata": {
        "id": "MYDjL0VVwREy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3**"
      ],
      "metadata": {
        "id": "yZHbnwavwqUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose 10 random users\n",
        "random_users = random.sample(list(ratings_df_600['user_id'].unique()), 10)\n",
        "\n",
        "# Randomly remove one rating for each user\n",
        "for user in random_users:\n",
        "    user_ratings = ratings_df_600[ratings_df_600['user_id'] == user]\n",
        "    random_rating = random.choice(user_ratings.index)\n",
        "    ratings_df_600_q3 = ratings_df_600.drop(random_rating)"
      ],
      "metadata": {
        "id": "er4X9wOnwt2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Surprise Dataset from the modified ratings DataFrame\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data_sup_q3 = Dataset.load_from_df( ratings_df_600_q3[['user_id', 'movie_id', 'rating']], reader)"
      ],
      "metadata": {
        "id": "5awSahkYxbWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the full training set\n",
        "trainset_userbased = data_sup_q3.build_full_trainset()"
      ],
      "metadata": {
        "id": "f1Z3RrDLxumd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the KNNBasic model\n",
        "model = KNNBasic( sim_options={'user_based': True})"
      ],
      "metadata": {
        "id": "T46V-y4byBRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the full training set\n",
        "model.fit(trainset_userbased)"
      ],
      "metadata": {
        "id": "Ykgi8JvfyEfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65351f4-bfeb-4101-d231-49466a962ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.knns.KNNBasic at 0x7acd72aec940>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store actual and predicted ratings\n",
        "actual_ratings = []\n",
        "predicted_ratings = []"
      ],
      "metadata": {
        "id": "rHXCWOdWyMXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the random users\n",
        "for user in random_users:\n",
        "    # Get the user's ratings\n",
        "    user_ratings =  ratings_df_600_q3[ ratings_df_600_q3['user_id'] == user]\n",
        "\n",
        "    # Get the item ID of the removed rating\n",
        "    removed_item = user_ratings['movie_id'].values[0]\n",
        "\n",
        "    # Get the actual rating for the removed item\n",
        "    actual_rating = user_ratings['rating'].values[0]\n",
        "    actual_ratings.append(actual_rating)\n",
        "\n",
        "    # Predict the rating for the removed item based on the neighbors\n",
        "    predicted_rating = model.predict(trainset_userbased.to_inner_uid(user), trainset_userbased.to_inner_iid(removed_item),\n",
        "                                     verbose=False).est\n",
        "    predicted_ratings.append(predicted_rating)"
      ],
      "metadata": {
        "id": "hnVwEl4_yTNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert actual_ratings and predicted_ratings to numpy arrays\n",
        "actual_ratings = np.array(actual_ratings)\n",
        "predicted_ratings = np.array(predicted_ratings)\n",
        "print(\"actual_ratings:\",actual_ratings )\n",
        "print(\"predicted_ratings:\",predicted_ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt-HzLs9yedi",
        "outputId": "4747e4bf-84c0-47f9-993c-b5e5dcf33d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual_ratings: [5 3 5 4 4 5 1 4 3 5]\n",
            "predicted_ratings: [4.16166211 3.80267872 4.31707624 4.32437825 3.60749584 3.5614604\n",
            " 4.14093946 3.8452836  3.19463671 3.28890021]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate MAE and RMSE\n",
        "mae = np.mean(np.abs(predicted_ratings - actual_ratings))\n",
        "rmse = np.sqrt(np.mean((predicted_ratings - actual_ratings) ** 2))\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfjp9g7ByhEU",
        "outputId": "c2c20be0-e845-4342-b7df-b61793ba827d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.9680754743720396\n",
            "RMSE: 1.3037388217701855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4**"
      ],
      "metadata": {
        "id": "Dj1_OD7tyl7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **based on k**"
      ],
      "metadata": {
        "id": "aLmeMUVnFlaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables for best k and best MAE/RMSE\n",
        "best_k = None\n",
        "best_mae_k = float('inf')\n",
        "best_rmse_k = float('inf')"
      ],
      "metadata": {
        "id": "BCA3EDecyprA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different values of k\n",
        "for k in [5, 15, 30, 50, 100]:\n",
        "    # Create the KNNBasic model with the current k value\n",
        "    model = KNNBasic(k=k, sim_options={'user_based': True})\n",
        "\n",
        "    # Train the model on the full training set\n",
        "    model.fit(trainset_userbased)\n",
        "\n",
        "    # Initialize lists to store actual and predicted ratings\n",
        "    actual_ratings = []\n",
        "    predicted_ratings = []\n",
        "\n",
        "    # Iterate over the random users\n",
        "    for user in random_users:\n",
        "        # Get the user's ratings\n",
        "        user_ratings =  ratings_df_600_q3[ ratings_df_600_q3['user_id'] == user]\n",
        "\n",
        "        # Get the item ID of the removed rating\n",
        "        removed_item = user_ratings['movie_id'].values[0]\n",
        "\n",
        "        # Get the actual rating for the removed item\n",
        "        actual_rating = user_ratings['rating'].values[0]\n",
        "        actual_ratings.append(actual_rating)\n",
        "\n",
        "        # Predict the rating for the removed item based on the neighbors\n",
        "        predicted_rating = model.predict(trainset_userbased.to_inner_uid(user), trainset_userbased.to_inner_iid(removed_item),\n",
        "                                         verbose=False).est\n",
        "        predicted_ratings.append(predicted_rating)\n",
        "\n",
        "    # Convert actual_ratings and predicted_ratings to numpy arrays\n",
        "    actual_ratings = np.array(actual_ratings)\n",
        "    predicted_ratings = np.array(predicted_ratings)\n",
        "\n",
        "    # Calculate MAE and RMSE\n",
        "    mae = np.mean(np.abs(predicted_ratings - actual_ratings))\n",
        "    rmse = np.sqrt(np.mean((predicted_ratings - actual_ratings) ** 2))\n",
        "\n",
        "    # Print the MAE and RMSE for the current k\n",
        "    print(f\"k = {k}\")\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    # Check if current k gives lower MAE and RMSE\n",
        "    if mae < best_mae_k and rmse < best_rmse_k:\n",
        "        best_k = k\n",
        "        best_mae_k = mae\n",
        "        best_rmse_k = rmse\n",
        "\n",
        "# Print the best k value, MAE, and RMSE\n",
        "print(\"Best k:\", best_k)\n",
        "print(\"Best MAE:\", best_mae_k)\n",
        "print(\"Best RMSE:\", best_rmse_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtfGgT9nywwz",
        "outputId": "66e3822a-63b5-4ee6-ad77-9ae0196991d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 5\n",
            "MAE: 0.84156496501267\n",
            "RMSE: 1.177584917362434\n",
            "-----------------------------\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 15\n",
            "MAE: 0.9649454044344192\n",
            "RMSE: 1.3165514480491627\n",
            "-----------------------------\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 30\n",
            "MAE: 0.9641604870119991\n",
            "RMSE: 1.3154927844717865\n",
            "-----------------------------\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 50\n",
            "MAE: 0.9773202613053666\n",
            "RMSE: 1.3014185580267446\n",
            "-----------------------------\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 100\n",
            "MAE: 1.0221232592363387\n",
            "RMSE: 1.3123826253126274\n",
            "-----------------------------\n",
            "Best k: 5\n",
            "Best MAE: 0.84156496501267\n",
            "Best RMSE: 1.177584917362434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calcualte Precsion , Recall and F1-score for user-based Collaborarive Filtering based on k."
      ],
      "metadata": {
        "id": "87IW6GvSIZGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from surprise import KNNBasic\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "# Load MovieLens ratings dataset\n",
        "ratings_df = pd.read_csv('movielens_ratings.csv')\n",
        "ratings_df = ratings_df.sample(frac=0.3, random_state=42)\n",
        "\n",
        "# Create Surprise Dataset\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(ratings_df[['user_id', 'movie_id', 'rating']], reader)\n",
        "\n",
        "# Train-test split\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize lists to store results for different K values\n",
        "results = []\n",
        "\n",
        "# Iterate over different K values\n",
        "for k in [5, 15, 30, 50, 100]:\n",
        "    # Create the KNNBasic model with the current k value\n",
        "    model = KNNBasic(k=k, sim_options={'user_based': True})\n",
        "\n",
        "    # Train the model on the full training set\n",
        "    model.fit(trainset_userbased)\n",
        "\n",
        "    # Initialize lists to store relevant items and recommended items\n",
        "    relevant_items = []\n",
        "    recommended_items = []\n",
        "\n",
        "    # Iterate over the test set\n",
        "    for user_id, movie_id, rating in testset:\n",
        "        # Get the relevant items (items rated 4 or higher by the user in the test set)\n",
        "        if rating >= 4:\n",
        "            relevant_items.append(movie_id)\n",
        "\n",
        "        # Predict the rating for the item based on the neighbors\n",
        "        predicted_rating = model.predict(user_id, movie_id).est\n",
        "\n",
        "        # Recommend the item if the predicted rating is 4 or higher\n",
        "        if predicted_rating >= 4:\n",
        "            recommended_items.append(movie_id)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    intersection = len(set(recommended_items) & set(relevant_items))\n",
        "    precision = intersection / len(recommended_items) if len(recommended_items) > 0 else 0.0\n",
        "    recall = intersection / len(relevant_items)\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    # Store the results for this K value\n",
        "    results.append((k, precision, recall, F1))\n",
        "\n",
        "# Sort the results based on F1-score in descending order\n",
        "results.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "# Print precision, recall, and F1-score for each K value\n",
        "for k, precision, recall, F1 in results:\n",
        "    print(\"K =\", k)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-Score:\", F1)\n",
        "    print()\n",
        "\n",
        "# Print the best precision, recall, and F1-score value\n",
        "best_k, best_precision, best_recall, best_F1 = results[0]\n",
        "print(\"Best K =\", best_k)\n",
        "print(\"Best Precision:\", best_precision)\n",
        "print(\"Best Recall:\", best_recall)\n",
        "print(\"Best F1-Score:\", best_F1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxBgRoKdEXD-",
        "outputId": "0b941186-7dd2-4696-d3af-598453a01395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "K = 5\n",
            "Precision: 0.11787324317157252\n",
            "Recall: 0.05165001161980014\n",
            "F1-Score: 0.07182677547063102\n",
            "\n",
            "K = 15\n",
            "Precision: 0.10391152675721814\n",
            "Recall: 0.04066930048803161\n",
            "F1-Score: 0.05845877612376559\n",
            "\n",
            "K = 30\n",
            "Precision: 0.09254696643055128\n",
            "Recall: 0.03491749941900999\n",
            "F1-Score: 0.05070446300514637\n",
            "\n",
            "K = 50\n",
            "Precision: 0.08363665136888748\n",
            "Recall: 0.030705321868463864\n",
            "F1-Score: 0.04491946793591433\n",
            "\n",
            "K = 100\n",
            "Precision: 0.0732170894967358\n",
            "Recall: 0.02573785730885429\n",
            "F1-Score: 0.038087049973132725\n",
            "\n",
            "Best K = 5\n",
            "Best Precision: 0.11787324317157252\n",
            "Best Recall: 0.05165001161980014\n",
            "Best F1-Score: 0.07182677547063102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **based on threshold**"
      ],
      "metadata": {
        "id": "DZF8ObD8FoiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of threshold intervals\n",
        "threshold_intervals = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]"
      ],
      "metadata": {
        "id": "6YxsTLObzVBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to store the best threshold, MAE, and RMSE\n",
        "best_threshold = None\n",
        "best_mae = float('inf')\n",
        "best_rmse = float('inf')"
      ],
      "metadata": {
        "id": "X0T9B-DnzYlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the threshold intervals\n",
        "for threshold_interval in threshold_intervals:\n",
        "    # Extract the lower and upper bounds of the threshold interval\n",
        "    lower_bound, upper_bound = threshold_interval\n",
        "\n",
        "    # Create a KNNBasic model with the current similarity threshold interval\n",
        "    model = KNNBasic(sim_options={'user_based': True})\n",
        "\n",
        "    # Train the model on the full dataset\n",
        "    model.fit(trainset_userbased)\n",
        "\n",
        "    # Initialize lists to store actual and predicted ratings\n",
        "    actual_ratings = []\n",
        "    predicted_ratings = []\n",
        "\n",
        "\n",
        "    # Iterate over the random users\n",
        "    for user in random_users:\n",
        "        # Get the user's ratings\n",
        "        user_ratings =  ratings_df_600_q3[ ratings_df_600_q3['user_id'] == user]\n",
        "\n",
        "        # Get the item ID of the removed rating\n",
        "        removed_item = user_ratings['movie_id'].values[0]\n",
        "\n",
        "        # Get the actual rating for the removed item\n",
        "        actual_rating = user_ratings[user_ratings['movie_id'] == removed_item]['rating'].values[0]\n",
        "\n",
        "        # Calculate Pearson correlation between the current user and other users\n",
        "        correlations = []\n",
        "        for other_user in  ratings_df_600_q3['user_id'].unique():\n",
        "            if other_user != user:\n",
        "                other_user_ratings =  ratings_df_600_q3[ ratings_df_600_q3['user_id'] == other_user]\n",
        "                common_ratings = user_ratings.merge(other_user_ratings, on='movie_id')\n",
        "                if len(common_ratings) > 1 and np.var(common_ratings['rating_x']) > 0 and np.var(common_ratings['rating_y']) > 0:\n",
        "                    correlation = pearsonr(common_ratings['rating_x'], common_ratings['rating_y'])[0]\n",
        "                    correlations.append(correlation)\n",
        "\n",
        "        # Filter neighbors based on similarity threshold\n",
        "        filtered_neighbors = [ ratings_df_600_q3['user_id'].unique()[i] for i, correlation in enumerate(correlations)\n",
        "                              if lower_bound <= correlation <= upper_bound]\n",
        "\n",
        "        # Predict the rating based on the filtered neighbors\n",
        "        predicted_rating = np.mean([model.predict(user, removed_item).est for user in filtered_neighbors])\n",
        "\n",
        "        # Append the actual and predicted ratings to the lists\n",
        "        actual_ratings.append(actual_rating)\n",
        "        predicted_ratings.append(predicted_rating)\n",
        "\n",
        "    # Convert actual_ratings and predicted_ratings to numpy arrays\n",
        "    actual_ratings = np.array(actual_ratings)\n",
        "    predicted_ratings = np.array(predicted_ratings)\n",
        "\n",
        "    # Calculate MAE and RMSE\n",
        "    mae = np.mean(np.abs(predicted_ratings - actual_ratings))\n",
        "    rmse = np.sqrt(np.mean((predicted_ratings - actual_ratings) ** 2))\n",
        "\n",
        "    # Check if current threshold interval gives lower MAE and RMSE\n",
        "    if mae < best_mae and rmse < best_rmse:\n",
        "        best_threshold = threshold_interval\n",
        "        best_mae = mae\n",
        "        best_rmse = rmse\n",
        "\n",
        "    # Print the MAE and RMSE for the current threshold interval\n",
        "    print(f\"Threshold Interval: {threshold_interval}\")\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print()\n",
        "\n",
        "# Print the best threshold interval, MAE, and RMSE\n",
        "print(\"Best Threshold Interval:\", best_threshold)\n",
        "print(\"Best MAE:\", best_mae)\n",
        "print(\"Best RMSE:\", best_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95WjT_iNza0v",
        "outputId": "a955134c-0e2f-4fce-edb2-63d1733a3e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.5, 0.6)\n",
            "MAE: 0.8667245581443492\n",
            "RMSE: 1.0350647228967733\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.6, 0.7)\n",
            "MAE: 0.8511456868920838\n",
            "RMSE: 1.016250531717755\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.7, 0.8)\n",
            "MAE: 0.8837178613981941\n",
            "RMSE: 1.0436629022800616\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.8, 0.9)\n",
            "MAE: 0.8807432752070318\n",
            "RMSE: 1.044277213295718\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.9, 1.0)\n",
            "MAE: 0.8651130848756582\n",
            "RMSE: 1.0303069170894592\n",
            "\n",
            "Best Threshold Interval: (0.6, 0.7)\n",
            "Best MAE: 0.8511456868920838\n",
            "Best RMSE: 1.016250531717755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculate Precsion , Recall and F1-score for user-based Collaborative Filtering based on threshold."
      ],
      "metadata": {
        "id": "F0yOMMuzNRec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "from surprise import KNNBasic\n",
        "\n",
        "# Load MovieLens ratings dataset\n",
        "ratings_df = pd.read_csv('movielens_ratings.csv')\n",
        "ratings_df = ratings_df.sample(frac=0.3, random_state=42)\n",
        "\n",
        "# Initialize variables for best threshold, precision, and recall\n",
        "best_threshold = None\n",
        "best_precision = 0.0\n",
        "best_recall = 0.0\n",
        "\n",
        "# Define the range of threshold intervals\n",
        "threshold_intervals = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
        "\n",
        "# Iterate over the threshold intervals\n",
        "for threshold_interval in threshold_intervals:\n",
        "    # Extract the lower and upper bounds of the threshold interval\n",
        "    lower_bound, upper_bound = threshold_interval\n",
        "\n",
        "    # Create a KNNBasic model with the current similarity threshold interval\n",
        "    model = KNNBasic(sim_options={'user_based': True})\n",
        "\n",
        "    # Train the model on the full dataset\n",
        "    #model.fit(ratings_df)\n",
        "    model.fit(trainset_userbased)\n",
        "\n",
        "    # Initialize lists to store relevant items and recommended items\n",
        "    relevant_items = []\n",
        "    recommended_items = []\n",
        "\n",
        "    # Iterate over the users\n",
        "    for user in random_users:  # Replace \"your_random_users_list\" with your list of random users\n",
        "        # Get the user's ratings\n",
        "        user_ratings = ratings_df[ratings_df['user_id'] == user]\n",
        "\n",
        "        # Get the item ID of the removed rating\n",
        "        removed_item = user_ratings['movie_id'].values[0]\n",
        "\n",
        "        # Get the actual rating for the removed item\n",
        "        actual_rating = user_ratings[user_ratings['movie_id'] == removed_item]['rating'].values[0]\n",
        "\n",
        "        # Calculate Pearson correlation between the current user and other users\n",
        "        correlations = []\n",
        "        for other_user in ratings_df['user_id'].unique():\n",
        "            if other_user != user:\n",
        "                other_user_ratings = ratings_df[ratings_df['user_id'] == other_user]\n",
        "                common_ratings = user_ratings.merge(other_user_ratings, on='movie_id')\n",
        "                if len(common_ratings) > 1 and np.var(common_ratings['rating_x']) > 0 and np.var(common_ratings['rating_y']) > 0:\n",
        "                    correlation = pearsonr(common_ratings['rating_x'], common_ratings['rating_y'])[0]\n",
        "                    correlations.append(correlation)\n",
        "\n",
        "        # Filter neighbors based on similarity threshold\n",
        "        filtered_neighbors = [ratings_df['user_id'].unique()[i] for i, correlation in enumerate(correlations)\n",
        "                              if lower_bound <= correlation <= upper_bound]\n",
        "\n",
        "        # Predict the rating based on the filtered neighbors\n",
        "        predicted_rating = np.mean([model.predict(user, removed_item).est for user in filtered_neighbors])\n",
        "\n",
        "        # Append the actual and predicted ratings to the lists\n",
        "        relevant_items.append(removed_item)\n",
        "        recommended_items.append(removed_item) if predicted_rating >= 4 else None\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    intersection = len(set(recommended_items) & set(relevant_items))\n",
        "    precision = intersection / len(recommended_items) if len(recommended_items) > 0 else 0.0\n",
        "    recall = intersection / len(relevant_items)\n",
        "\n",
        "    # Check if current threshold interval gives higher precision and recall\n",
        "    if precision > best_precision and recall > best_recall:\n",
        "        best_threshold = threshold_interval\n",
        "        best_precision = precision\n",
        "        best_recall = recall\n",
        "\n",
        "    # Print the precision and recall for the current threshold interval\n",
        "    print(f\"Threshold Interval: {threshold_interval}\")\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print()\n",
        "\n",
        "# Print the best threshold interval, precision, and recall\n",
        "print(\"Best Threshold Interval:\", best_threshold)\n",
        "print(\"Best Precision:\", best_precision)\n",
        "print(\"Best Recall:\", best_recall)\n",
        "print(\"F1-Score:\", 2 * (best_precision * best_recall) / (best_precision + best_recall) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "656y2D25Cd7b",
        "outputId": "b650c759-bb41-4b59-ffee-5f4eda6fd51a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.5, 0.6)\n",
            "Precision: 1.0\n",
            "Recall: 0.1\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold Interval: (0.6, 0.7)\n",
            "Precision: 1.0\n",
            "Recall: 0.1\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.7, 0.8)\n",
            "Precision: 1.0\n",
            "Recall: 0.1\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold Interval: (0.8, 0.9)\n",
            "Precision: 1.0\n",
            "Recall: 0.1\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.9, 1.0)\n",
            "Precision: 1.0\n",
            "Recall: 0.1\n",
            "\n",
            "Best Threshold Interval: (0.5, 0.6)\n",
            "Best Precision: 1.0\n",
            "Best Recall: 0.1\n",
            "F1-Score: 0.18181818181818182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5**"
      ],
      "metadata": {
        "id": "p9z0YWmUz66R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose 10 random items\n",
        "random_items = random.sample(list(ratings_df_600['movie_id'].unique()), 10)\n",
        "# Randomly remove one rating for each item\n",
        "for item in random_items:\n",
        "    item_ratings = ratings_df_600[ratings_df_600['movie_id'] == item]\n",
        "    if len(item_ratings) > 0:\n",
        "        random_rating = random.choice(item_ratings.index)\n",
        "        ratings_df_600_q5 = ratings_df_600.drop(random_rating)"
      ],
      "metadata": {
        "id": "BFiWL7Xzz-s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Surprise Dataset from the modified ratings DataFrame\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data_sup_q5 = Dataset.load_from_df(ratings_df_600_q5[['user_id', 'movie_id', 'rating']], reader)"
      ],
      "metadata": {
        "id": "X_CIue5i0z4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the full training set\n",
        "trainset_itembased = data_sup_q5.build_full_trainset()"
      ],
      "metadata": {
        "id": "FzltpSqz08uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the KNNBasic model\n",
        "model = KNNBasic( sim_options={'user_based': False})"
      ],
      "metadata": {
        "id": "bX3xOjM21Fr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the full training set\n",
        "model.fit(trainset_itembased)"
      ],
      "metadata": {
        "id": "1TmCtpDS1N4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8144826e-52c7-4187-a0fe-29f73094e662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.knns.KNNBasic at 0x7a82f52f2860>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store actual and predicted ratings\n",
        "actual_ratings = []\n",
        "predicted_ratings = []"
      ],
      "metadata": {
        "id": "5dy_S9dR1Yx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the random items\n",
        "for item in random_items:\n",
        "    # Get the item's ratings\n",
        "    item_ratings = ratings_df_600_q5[ratings_df_600_q5['movie_id'] == item]\n",
        "\n",
        "    if len(item_ratings) > 0:\n",
        "        # Get the user ID of the removed rating\n",
        "        removed_user = item_ratings['user_id'].values[0]\n",
        "\n",
        "        # Get the actual rating for the removed user-item pair\n",
        "        actual_rating = item_ratings['rating'].values[0]\n",
        "        actual_ratings.append(actual_rating)\n",
        "\n",
        "        # Predict the rating for the removed user-item pair based on similar items\n",
        "        predicted_rating = model.predict(trainset_itembased.to_inner_uid(removed_user), trainset_itembased.to_inner_iid(item),\n",
        "                                         verbose=False).est\n",
        "        predicted_ratings.append(predicted_rating)"
      ],
      "metadata": {
        "id": "Omvw5ewV1bI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(actual_ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2XtfrJU1mAY",
        "outputId": "475d9fdf-cc7a-4046-b4ad-829debf6b3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 4, 4, 5, 4, 4, 3, 5, 5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_ratings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkltpW771ofu",
        "outputId": "e2b92498-6f8c-4b9a-f500-bdb5419f694f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.977151496577971, 3.4210247534879903, 3.785714285714285, 3.312794608062914, 2.9889518523618244, 3.5069466959740225, 3.4981639252824497, 3.5736059560099265, 3.290827615652893, 2.952762958923318]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert actual_ratings and predicted_ratings to numpy arrays\n",
        "actual_ratings = np.array(actual_ratings)\n",
        "predicted_ratings = np.array(predicted_ratings)\n",
        "\n",
        "# Calculate MAE and RMSE\n",
        "mae = np.mean(np.abs(predicted_ratings - actual_ratings))\n",
        "rmse = np.sqrt(np.mean((predicted_ratings - actual_ratings) ** 2))"
      ],
      "metadata": {
        "id": "MgNuO8BB1q9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X3GyGJy1vz9",
        "outputId": "9c5c9a74-b674-4a8a-a85c-adaad0aae203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.9688383702517307\n",
            "RMSE: 1.088121070525213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6**"
      ],
      "metadata": {
        "id": "SbIHJ3VF10Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **based on k**"
      ],
      "metadata": {
        "id": "l7zlWopSFu_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to store MAE and RMSE for each k\n",
        "mae_values = []\n",
        "rmse_values = []\n",
        "\n",
        "# Initialize variables to store the best k and corresponding MAE/RMSE values\n",
        "best_k = None\n",
        "best_mae = float('inf')\n",
        "best_rmse = float('inf')"
      ],
      "metadata": {
        "id": "hRl12rD_13y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different values of k\n",
        "for k in [5, 15, 30, 50, 100]:\n",
        "    # Create the KNNBasic model with the current k value\n",
        "    model = KNNBasic(k=k, sim_options={'user_based': False})\n",
        "\n",
        "    # Train the model on the full training set\n",
        "    model.fit(trainset_itembased)\n",
        "\n",
        "    # Initialize lists to store actual and predicted ratings\n",
        "    actual_ratings = []\n",
        "    predicted_ratings = []\n",
        "\n",
        "    # Iterate over the random users\n",
        "    for item in random_items:\n",
        "        # Get the item's ratings\n",
        "        item_ratings = ratings_df_600_q5[ratings_df_600_q5['movie_id'] == item]\n",
        "\n",
        "        if len(item_ratings) > 0:\n",
        "            # Get the user ID of the removed rating\n",
        "            removed_user = item_ratings['user_id'].values[0]\n",
        "\n",
        "            # Get the actual rating for the removed user-item pair\n",
        "            actual_rating = item_ratings['rating'].values[0]\n",
        "            actual_ratings.append(actual_rating)\n",
        "\n",
        "            # Predict the rating for the removed user-item pair based on similar items\n",
        "            predicted_rating = model.predict(trainset_itembased.to_inner_uid(removed_user), trainset_itembased.to_inner_iid(item),\n",
        "                                         verbose=False).est\n",
        "            predicted_ratings.append(predicted_rating)\n",
        "\n",
        "\n",
        "    # Convert actual_ratings and predicted_ratings to numpy arrays\n",
        "    actual_ratings = np.array(actual_ratings)\n",
        "    predicted_ratings = np.array(predicted_ratings)\n",
        "\n",
        "    # Calculate MAE and RMSE\n",
        "    mae = np.mean(np.abs(predicted_ratings - actual_ratings))\n",
        "    rmse = np.sqrt(np.mean((predicted_ratings - actual_ratings) ** 2))\n",
        "\n",
        "    # Print the MAE and RMSE for the current k\n",
        "    print(f\"k = {k}\")\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    # Check if current k gives lower MAE and RMSE\n",
        "    if mae < best_mae and rmse < best_rmse:\n",
        "        best_k = k\n",
        "        best_mae = mae\n",
        "        best_rmse = rmse\n",
        "\n",
        "# Print the best k value, MAE, and RMSE\n",
        "print(\"Best k:\", best_k)\n",
        "print(\"Best MAE:\", best_mae)\n",
        "print(\"Best RMSE:\", best_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b5i3u6119E5",
        "outputId": "609f26f5-675b-4297-c4d0-b84e8110f00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 5\n",
            "MAE: 1.1326088811124149\n",
            "RMSE: 1.2960101644026878\n",
            "-----------------------------\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 15\n",
            "MAE: 1.0226992448814962\n",
            "RMSE: 1.238678620975884\n",
            "-----------------------------\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 30\n",
            "MAE: 0.970064178035835\n",
            "RMSE: 1.1709729634492743\n",
            "-----------------------------\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 50\n",
            "MAE: 0.925717307003028\n",
            "RMSE: 1.1377820631347404\n",
            "-----------------------------\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "k = 100\n",
            "MAE: 0.9519636033429556\n",
            "RMSE: 1.1759477897769224\n",
            "-----------------------------\n",
            "Best k: 50\n",
            "Best MAE: 0.925717307003028\n",
            "Best RMSE: 1.1377820631347404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate Precision, Recall and F1-score for Item-based collaborative filtering based on k."
      ],
      "metadata": {
        "id": "QucgebgoS0FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from surprise import KNNBasic\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "# Load MovieLens ratings dataset\n",
        "ratings_df = pd.read_csv('movielens_ratings.csv')\n",
        "ratings_df = ratings_df.sample(frac=0.3, random_state=42)\n",
        "\n",
        "# Create Surprise Dataset\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(ratings_df[['user_id', 'movie_id', 'rating']], reader)\n",
        "\n",
        "# Train-test split\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize lists to store results for different K values\n",
        "results = []\n",
        "\n",
        "# Iterate over different K values\n",
        "for k in [5, 15, 30, 50, 100]:\n",
        "    # Create the KNNBasic model with the current k value\n",
        "    model = KNNBasic(k=k, sim_options={'user_based': False})  # Use item-based collaborative filtering\n",
        "\n",
        "    # Train the model on the full training set\n",
        "    model.fit(trainset)\n",
        "\n",
        "    # Initialize lists to store relevant items and recommended items\n",
        "    relevant_items = []\n",
        "    recommended_items = []\n",
        "\n",
        "    # Iterate over the test set\n",
        "    for user_id, movie_id, rating in testset:\n",
        "        # Get the relevant items (items rated 4 or higher by the user in the test set)\n",
        "        if rating >= 4:\n",
        "            relevant_items.append(movie_id)\n",
        "\n",
        "        # Predict the rating for the item based on the neighbors\n",
        "        predicted_rating = model.predict(user_id, movie_id).est\n",
        "\n",
        "        # Recommend the item if the predicted rating is 4 or higher\n",
        "        if predicted_rating >= 4:\n",
        "            recommended_items.append(movie_id)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    intersection = len(set(recommended_items) & set(relevant_items))\n",
        "    precision = intersection / len(recommended_items) if len(recommended_items) > 0 else 0.0\n",
        "    recall = intersection / len(relevant_items)\n",
        "    F1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    # Store the results for this K value\n",
        "    results.append((k, precision, recall, F1))\n",
        "\n",
        "# Sort the results based on F1-score in descending order\n",
        "results.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "# Print precision, recall, and F1-score for each K value\n",
        "for k, precision, recall, F1 in results:\n",
        "    print(\"K =\", k)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-Score:\", F1)\n",
        "    print()\n",
        "\n",
        "# Print the best precision, recall, and F1-score value\n",
        "best_k, best_precision, best_recall, best_F1 = results[0]\n",
        "print(\"Best K =\", best_k)\n",
        "print(\"Best Precision:\", best_precision)\n",
        "print(\"Best Recall:\", best_recall)\n",
        "print(\"Best F1-Score:\", best_F1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RahSJgJcSmMg",
        "outputId": "7503f729-2979-458c-c526-042dfd7fa898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "K = 100\n",
            "Precision: 0.17071590540978296\n",
            "Recall: 0.06123634673483616\n",
            "F1-Score: 0.09013939964081073\n",
            "\n",
            "K = 5\n",
            "Precision: 0.1335527455395808\n",
            "Recall: 0.06719149430629794\n",
            "F1-Score: 0.08940339756102274\n",
            "\n",
            "K = 50\n",
            "Precision: 0.15928262025989817\n",
            "Recall: 0.060887752730653034\n",
            "F1-Score: 0.08809869070886663\n",
            "\n",
            "K = 30\n",
            "Precision: 0.15280322462440454\n",
            "Recall: 0.0605682082268185\n",
            "F1-Score: 0.08675029644885476\n",
            "\n",
            "K = 15\n",
            "Precision: 0.1462570914625709\n",
            "Recall: 0.06141064373692773\n",
            "F1-Score: 0.08650108433241949\n",
            "\n",
            "Best K = 100\n",
            "Best Precision: 0.17071590540978296\n",
            "Best Recall: 0.06123634673483616\n",
            "Best F1-Score: 0.09013939964081073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **based on threshold**"
      ],
      "metadata": {
        "id": "v-W1YkieFymT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of threshold intervals\n",
        "threshold_intervals = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]"
      ],
      "metadata": {
        "id": "lswX6TaI8ZDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to store the best threshold, MAE, and RMSE\n",
        "best_threshold = None\n",
        "best_mae = float('inf')\n",
        "best_rmse = float('inf')"
      ],
      "metadata": {
        "id": "nlo7bQgK8f4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the threshold intervals\n",
        "for threshold_interval in threshold_intervals:\n",
        "    # Extract the lower and upper bounds of the threshold interval\n",
        "    lower_bound, upper_bound = threshold_interval\n",
        "\n",
        "    # Create a KNNBasic model with the current similarity threshold interval\n",
        "    model = KNNBasic(sim_options={'user_based': False})\n",
        "\n",
        "    # Train the model on the full dataset\n",
        "    model.fit(trainset_itembased)\n",
        "\n",
        "    # Initialize lists to store actual and predicted ratings\n",
        "    actual_ratings = []\n",
        "    predicted_ratings = []\n",
        "\n",
        "    # Iterate over the random items\n",
        "    for item in random_items:\n",
        "        # Get the ratings for the current item\n",
        "        item_ratings = ratings_df_600_q5[ratings_df_600_q5['movie_id'] == item]\n",
        "\n",
        "        # Randomly select a user's rating for the current item to remove\n",
        "        removed_rating = random.choice(item_ratings['rating'].tolist())\n",
        "\n",
        "        # Get the actual rating for the removed item\n",
        "        actual_rating = removed_rating\n",
        "\n",
        "        # Calculate Pearson correlation between the current item and other items\n",
        "        correlations = []\n",
        "        for other_item in ratings_df_600_q5['movie_id'].unique():\n",
        "            if other_item != item:\n",
        "                other_item_ratings = ratings_df_600_q5[ratings_df_600_q5['movie_id'] == other_item]\n",
        "                common_ratings = item_ratings.merge(other_item_ratings, on='user_id')\n",
        "                if len(common_ratings) > 1 and np.var(common_ratings['rating_x']) > 0 and np.var(common_ratings['rating_y']) > 0:\n",
        "                    correlation = pearsonr(common_ratings['rating_x'], common_ratings['rating_y'])[0]\n",
        "                    correlations.append(correlation)\n",
        "\n",
        "        # Filter neighbors based on similarity threshold\n",
        "        filtered_neighbors = [ratings_df_600_q5['movie_id'].unique()[i] for i, correlation in enumerate(correlations)\n",
        "                              if lower_bound <= correlation <= upper_bound]\n",
        "\n",
        "        # Predict the rating based on the filtered neighbors\n",
        "        if item in ratings_df_600_q5['movie_id'].unique() and len(filtered_neighbors) > 0:\n",
        "            predicted_rating = np.mean([model.predict(user, item).est for user in filtered_neighbors])\n",
        "        else:\n",
        "            predicted_rating = np.nan\n",
        "\n",
        "        # Append the actual and predicted ratings to the lists\n",
        "        actual_ratings.append(actual_rating)\n",
        "        predicted_ratings.append(predicted_rating)\n",
        "\n",
        "    # Convert actual_ratings and predicted_ratings to numpy arrays\n",
        "    actual_ratings = np.array(actual_ratings)\n",
        "    predicted_ratings = np.array(predicted_ratings)\n",
        "\n",
        "    # Calculate MAE and RMSE\n",
        "    mae = np.mean(np.abs(predicted_ratings - actual_ratings))\n",
        "    rmse = np.sqrt(np.mean((predicted_ratings - actual_ratings) ** 2))\n",
        "\n",
        "    # Check if current threshold interval gives lower MAE and RMSE\n",
        "    if mae < best_mae and rmse < best_rmse:\n",
        "        best_threshold = threshold_interval\n",
        "        best_mae = mae\n",
        "        best_rmse = rmse\n",
        "\n",
        "    # Print the MAE and RMSE for the current threshold interval\n",
        "    print(f\"Threshold Interval: {threshold_interval}\")\n",
        "    print(\"MAE:\", mae)\n",
        "    print(\"RMSE:\", rmse)\n",
        "    print()\n",
        "\n",
        "# Print the best threshold interval, MAE, and RMSE\n",
        "print(\"Best Threshold Interval:\", best_threshold)\n",
        "print(\"Best MAE:\", best_mae)\n",
        "print(\"Best RMSE:\", best_rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t7_JepWLjOl",
        "outputId": "8ffe289f-a796-4ff7-b8b2-cb6ee2cb16ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.5, 0.6)\n",
            "MAE: 0.9603825191107758\n",
            "RMSE: 1.2694832636575155\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.6, 0.7)\n",
            "MAE: 1.2566903959592348\n",
            "RMSE: 1.5201543619312323\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.7, 0.8)\n",
            "MAE: 1.023826950300771\n",
            "RMSE: 1.111398112545164\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.8, 0.9)\n",
            "MAE: 1.034777776352649\n",
            "RMSE: 1.2015860617656238\n",
            "\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Threshold Interval: (0.9, 1.0)\n",
            "MAE: 0.8713997759295735\n",
            "RMSE: 1.2449384465044666\n",
            "\n",
            "Best Threshold Interval: (0.7, 0.8)\n",
            "Best MAE: 0.925717307003028\n",
            "Best RMSE: 1.1377820631347404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Precision, Recall, and F1-Score for Item-based collaborative filtering based on threshold"
      ],
      "metadata": {
        "id": "xoeEPgSqVFcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "from surprise import KNNBasic\n",
        "\n",
        "# Load MovieLens ratings dataset\n",
        "ratings_df = pd.read_csv('movielens_ratings.csv')\n",
        "ratings_df = ratings_df.sample(frac=0.3, random_state=42)\n",
        "\n",
        "# Define the range of threshold intervals\n",
        "threshold_intervals = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
        "\n",
        "# Initialize variables to store the best threshold, MAE, RMSE, precision, recall, and F1-score\n",
        "best_threshold = None\n",
        "best_mae = float('inf')\n",
        "best_rmse = float('inf')\n",
        "best_precision = 0.0\n",
        "best_recall = 0.0\n",
        "best_f1_score = 0.0\n",
        "\n",
        "# Iterate over the threshold intervals\n",
        "for threshold_interval in threshold_intervals:\n",
        "    # Extract the lower and upper bounds of the threshold interval\n",
        "    lower_bound, upper_bound = threshold_interval\n",
        "\n",
        "    # Create a KNNBasic model with the current similarity threshold interval\n",
        "    model = KNNBasic(sim_options={'user_based': False})\n",
        "\n",
        "    # Train the model on the full dataset\n",
        "    model.fit(trainset_itembased)\n",
        "\n",
        "    # Initialize lists to store actual and predicted ratings\n",
        "    actual_ratings = []\n",
        "    predicted_ratings = []\n",
        "    relevant_items = []\n",
        "    recommended_items = []\n",
        "\n",
        "    # Iterate over the random items\n",
        "    for item in random_items:\n",
        "        # Get the ratings for the current item\n",
        "        item_ratings = ratings_df[ratings_df['movie_id'] == item]\n",
        "\n",
        "        # Randomly select a user's rating for the current item to remove\n",
        "        removed_rating = random.choice(item_ratings['rating'].tolist())\n",
        "\n",
        "        # Get the actual rating for the removed item\n",
        "        actual_rating = removed_rating\n",
        "\n",
        "        # Calculate Pearson correlation between the current item and other items\n",
        "        correlations = []\n",
        "        for other_item in ratings_df['movie_id'].unique():\n",
        "            if other_item != item:\n",
        "                other_item_ratings = ratings_df[ratings_df['movie_id'] == other_item]\n",
        "                common_ratings = item_ratings.merge(other_item_ratings, on='user_id')\n",
        "                if len(common_ratings) > 1 and np.var(common_ratings['rating_x']) > 0 and np.var(common_ratings['rating_y']) > 0:\n",
        "                    correlation = pearsonr(common_ratings['rating_x'], common_ratings['rating_y'])[0]\n",
        "                    correlations.append(correlation)\n",
        "\n",
        "        # Filter neighbors based on similarity threshold\n",
        "        filtered_neighbors = [ratings_df['movie_id'].unique()[i] for i, correlation in enumerate(correlations)\n",
        "                              if lower_bound <= correlation <= upper_bound]\n",
        "\n",
        "        # Predict the rating based on the filtered neighbors\n",
        "        if item in ratings_df['movie_id'].unique() and len(filtered_neighbors) > 0:\n",
        "            predicted_rating = np.mean([model.predict(user, item).est for user in filtered_neighbors])\n",
        "        else:\n",
        "            predicted_rating = np.nan\n",
        "\n",
        "        # Append the actual and predicted ratings to the lists\n",
        "        actual_ratings.append(actual_rating)\n",
        "        predicted_ratings.append(predicted_rating)\n",
        "\n",
        "        # Append the relevant and recommended items to the lists\n",
        "        relevant_items.append(item)\n",
        "        recommended_items.append(item) if predicted_rating >= 4 else None\n",
        "\n",
        "    # Convert actual_ratings and predicted_ratings to numpy arrays\n",
        "    actual_ratings = np.array(actual_ratings)\n",
        "    predicted_ratings = np.array(predicted_ratings)\n",
        "\n",
        "    # Calculate MAE and RMSE\n",
        "    mae = np.mean(np.abs(predicted_ratings - actual_ratings))\n",
        "    rmse = np.sqrt(np.mean((predicted_ratings - actual_ratings) ** 2))\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    intersection = len(set(recommended_items) & set(relevant_items))\n",
        "    precision = intersection / len(recommended_items) if len(recommended_items) > 0 else 0.0\n",
        "    recall = intersection / len(relevant_items) if len(relevant_items) > 0 else 0.0\n",
        "\n",
        "    # Calculate F1-score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    # Check if current threshold interval gives lower MAE and RMSE and higher precision, recall, and F1-score\n",
        "    if mae < best_mae and rmse < best_rmse and precision > best_precision and recall > best_recall and f1_score > best_f1_score:\n",
        "        best_threshold = threshold_interval\n",
        "        best_mae = mae\n",
        "        best_rmse = rmse\n",
        "        best_precision = precision\n",
        "        best_recall = recall\n",
        "        best_f1_score = f1_score\n",
        "        # Print the MAEand RMSE for the current threshold interval, as well as precision, recall, and F1-score:\n",
        "    #print(f\"Threshold Interval: {threshold_interval}\")\n",
        "    #print(\"MAE:\", mae)\n",
        "    #print(\"RMSE:\", rmse)\n",
        "    #print(\"Precision:\", precision)\n",
        "    #print(\"Recall:\", recall)\n",
        "    #print(\"F1-score:\", f1_score)\n",
        "    #print()\n",
        "\n",
        "# Print the best threshold interval, MAE, RMSE, precision, recall, and F1-score\n",
        "print(\"Best Threshold Interval:\", best_threshold)\n",
        "print(\"Best MAE:\", best_mae)\n",
        "print(\"Best RMSE:\", best_rmse)\n",
        "print(\"Best Precision:\", best_precision)\n",
        "print(\"Best Recall:\", best_recall)\n",
        "print(\"Best F1-score:\", best_f1_score)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPNA6W_yoUsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95bc352-9a35-41f4-9332-8754ade98062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Best Threshold Interval: (0.5, 0.6)\n",
            "Best MAE: 1.1985449238793235\n",
            "Best RMSE: 1.324398719332224\n",
            "Best Precision: 1.0\n",
            "Best Recall: 0.2\n",
            "Best F1-score: 0.33333333333333337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7**"
      ],
      "metadata": {
        "id": "cFKi9qd-A4f3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recommendations based on best values of K**"
      ],
      "metadata": {
        "id": "lTYaegHCtPh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of items to recommend\n",
        "num_recommendations = 5\n",
        "\n",
        "# Define the value of K for user-based collaborative filtering\n",
        "user_based_K = 15\n",
        "\n",
        "# Define the value of K for item-based collaborative filtering\n",
        "item_based_K = 100\n",
        "\n",
        "# Randomly select a user\n",
        "random_user = random.choice(ratings_df_600['user_id'].unique())\n",
        "\n",
        "# Create a defaultdict to store the user's ratings\n",
        "user_ratings = defaultdict(int)\n",
        "\n",
        "# Iterate over the ratings and populate the user_ratings dictionary\n",
        "for _, row in ratings_df_600[ratings_df_600['user_id'] == random_user].iterrows():\n",
        "    movie_id = row['movie_id']\n",
        "    rating = row['rating']\n",
        "    user_ratings[movie_id] = rating\n",
        "\n",
        "# Create a Reader object to parse the ratings DataFrame\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Load the ratings DataFrame into a Surprise Dataset\n",
        "data = Dataset.load_from_df(ratings_df_600[['user_id', 'movie_id', 'rating']], reader)\n",
        "\n",
        "# Build the full trainset\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "# Create the KNNBasic model with user-based collaborative filtering\n",
        "user_based_model = KNNBasic(k=user_based_K, sim_options={'user_based': True})\n",
        "\n",
        "# Train the user-based model on the full training set\n",
        "user_based_model.fit(trainset)\n",
        "\n",
        "# Get the inner user ID for the random user\n",
        "inner_user_id = trainset.to_inner_uid(random_user)\n",
        "\n",
        "# Use the trained user-based model to get the top-K similar users to the random user\n",
        "similar_users = user_based_model.get_neighbors(inner_user_id, k=user_based_K)\n",
        "\n",
        "# Create a defaultdict to store the aggregated ratings from similar users\n",
        "aggregated_ratings = defaultdict(float)\n",
        "similar_users_count = 0\n",
        "\n",
        "# Iterate over the similar users for user-based collaborative filtering\n",
        "for user_id in similar_users:\n",
        "    # Convert from inner user ID to raw user ID\n",
        "    raw_user_id = trainset.to_raw_uid(user_id)\n",
        "\n",
        "    # Create a defaultdict to store the ratings of the similar user\n",
        "    similar_user_ratings = defaultdict(int)\n",
        "\n",
        "    # Iterate over the ratings and populate the similar_user_ratings dictionary\n",
        "    for _, row in ratings_df_600[ratings_df_600['user_id'] == raw_user_id].iterrows():\n",
        "        movie_id = row['movie_id']\n",
        "        rating = row['rating']\n",
        "        similar_user_ratings[movie_id] = rating\n",
        "\n",
        "    # If the similar user has ratings\n",
        "    if similar_user_ratings:\n",
        "        # Iterate over the similar user's ratings\n",
        "        for movie_id, rating in similar_user_ratings.items():\n",
        "            # Add the rating to the aggregated ratings\n",
        "            aggregated_ratings[movie_id] += rating\n",
        "\n",
        "        similar_users_count += 1\n",
        "\n",
        "# Calculate the average ratings from similar users for user-based collaborative filtering\n",
        "average_user_ratings = {movie_id: rating / similar_users_count for movie_id, rating in aggregated_ratings.items()}\n",
        "\n",
        "# Sort the average user ratings in descending order\n",
        "sorted_user_ratings = sorted(average_user_ratings.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top-N recommended items for user-based collaborative filtering\n",
        "user_based_recommendations = [item_id for item_id, _ in sorted_user_ratings[:num_recommendations]]\n",
        "\n",
        "# Create the KNNBasic model with item-based collaborative filtering\n",
        "item_based_model = KNNBasic(k=item_based_K, sim_options={'user_based': False})\n",
        "\n",
        "# Train the item-based model on the full training set\n",
        "item_based_model.fit(trainset)\n",
        "\n",
        "# Get the inner item ID for the items rated by the random user\n",
        "inner_item_ids = [trainset.to_inner_iid(movie_id) for movie_id in user_ratings.keys()]\n",
        "\n",
        "# Use the trained item-based model to get the top-K similar items for the rated items\n",
        "similar_items = []\n",
        "for inner_item_id in inner_item_ids:\n",
        "    similar_items.extend(item_based_model.get_neighbors(inner_item_id, k=item_based_K))\n",
        "\n",
        "# Filter out the items that have already been rated by the user\n",
        "similar_items = [item_id for item_id in similar_items if trainset.to_raw_iid(item_id) not in user_ratings]\n",
        "\n",
        "# Create a defaultdict to store the aggregated ratings from similar items\n",
        "aggregated_item_ratings = defaultdict(float)\n",
        "similar_items_count = 0\n",
        "\n",
        "# Iterate over the similar items for item-based collaborative filtering\n",
        "for item_id in similar_items:\n",
        "    # Convert from inner item ID to raw item ID\n",
        "    raw_item_id = trainset.to_raw_iid(item_id)\n",
        "\n",
        "    # Get the rating of the similar item\n",
        "    rating = user_ratings.get(raw_item_id, 0)\n",
        "\n",
        "    # Add the rating to the aggregated ratings\n",
        "    aggregated_item_ratings[raw_item_id] += rating\n",
        "\n",
        "    similar_items_count += 1\n",
        "\n",
        "# Calculate the average ratings from similar items for```python\n",
        "# Calculate the average ratings from similar items for item-based collaborative filtering\n",
        "average_item_ratings = {item_id: rating / similar_items_count for item_id, rating in aggregated_item_ratings.items()}\n",
        "\n",
        "# Sort the average item ratings in descending order\n",
        "sorted_item_ratings = sorted(average_item_ratings.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top-N recommended items for item-based collaborative filtering\n",
        "item_based_recommendations = [item_id for item_id, _ in sorted_item_ratings[:num_recommendations]]\n",
        "\n",
        "# Print the recommendations for user-based collaborative filtering\n",
        "print(f\"Recommendations for User {random_user} (User-based Collaborative Filtering):\")\n",
        "for item_id in user_based_recommendations:\n",
        "    print(f\"- Movie ID: {item_id}\")\n",
        "\n",
        "# Print the recommendations for item-based collaborative filtering\n",
        "print(f\"\\nRecommendations for User {random_user} (Item-based Collaborative Filtering):\")\n",
        "for item_id in item_based_recommendations:\n",
        "    print(f\"- Movie ID: {item_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap_nc1lFA8TI",
        "outputId": "3eb25cdc-cee1-4abd-ca52-a7fd92337098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Recommendations for User 2447 (User-based Collaborative Filtering):\n",
            "- Movie ID: 593\n",
            "- Movie ID: 2858\n",
            "- Movie ID: 2028\n",
            "- Movie ID: 1617\n",
            "- Movie ID: 318\n",
            "\n",
            "Recommendations for User 2447 (Item-based Collaborative Filtering):\n",
            "- Movie ID: 2544\n",
            "- Movie ID: 3282\n",
            "- Movie ID: 1871\n",
            "- Movie ID: 1058\n",
            "- Movie ID: 1471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recommendations based on best threshold intervals**"
      ],
      "metadata": {
        "id": "FkQr4TeBtemo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of items to recommend\n",
        "num_recommendations = 5\n",
        "\n",
        "# Define the threshold intervals for user-based and item-based collaborative filtering\n",
        "user_based_threshold_interval = (0.7, 0.8)\n",
        "item_based_threshold_interval = (0.8, 0.9)\n",
        "\n",
        "# Randomly select a user\n",
        "random_user = random.choice(ratings_df_600['user_id'].unique())\n",
        "\n",
        "# Create a defaultdict to store the user's ratings\n",
        "user_ratings = defaultdict(int)\n",
        "\n",
        "# Iterate over the ratings and populate the user_ratings dictionary\n",
        "for _, row in ratings_df_600[ratings_df_600['user_id'] == random_user].iterrows():\n",
        "    movie_id = row['movie_id']\n",
        "    rating = row['rating']\n",
        "    user_ratings[movie_id] = rating\n",
        "\n",
        "# Create a Reader object to parse the ratings DataFrame\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Load the ratings DataFrame into a Surprise Dataset\n",
        "data = Dataset.load_from_df(ratings_df_600[['user_id', 'movie_id', 'rating']], reader)\n",
        "\n",
        "# Build the full trainset\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "# Create the KNNBasic model with user-based collaborative filtering\n",
        "user_based_model = KNNBasic(sim_options={'user_based': True})\n",
        "\n",
        "# Train the user-based model on the full training set\n",
        "user_based_model.fit(trainset)\n",
        "\n",
        "# Get the inner user ID for the random user\n",
        "inner_user_id = trainset.to_inner_uid(random_user)\n",
        "\n",
        "# Use the trained user-based model to get the similarities between the random user and other users\n",
        "user_similarities = user_based_model.sim[inner_user_id]\n",
        "\n",
        "# Create a defaultdict to store the aggregated ratings from similar users\n",
        "aggregated_ratings_user = defaultdict(float)\n",
        "similar_users_count = 0\n",
        "\n",
        "# Iterate over the user similarities for user-based collaborative filtering\n",
        "for user_id, similarity in enumerate(user_similarities):\n",
        "    # Convert from inner user ID to raw user ID\n",
        "    raw_user_id = trainset.to_raw_uid(user_id)\n",
        "\n",
        "    # Create a defaultdict to store the ratings of the similar user\n",
        "    similar_user_ratings = defaultdict(int)\n",
        "\n",
        "    # Iterate over the ratings and populate the similar_user_ratings dictionary\n",
        "    for _, row in ratings_df_600[ratings_df_600['user_id'] == raw_user_id].iterrows():\n",
        "        movie_id = row['movie_id']\n",
        "        rating = row['rating']\n",
        "        similar_user_ratings[movie_id] = rating\n",
        "\n",
        "    # Calculate the intersection of movies rated by the random user and the similar user\n",
        "    common_movies_user = set(user_ratings.keys()) & set(similar_user_ratings.keys())\n",
        "\n",
        "\n",
        "    # If there are at least two common movies\n",
        "    if len(common_movies_user) >= 2:\n",
        "        # Get the ratings for the common movies\n",
        "        user_ratings_common = [user_ratings[movie_id] for movie_id in common_movies_user]\n",
        "        similar_user_ratings_common = [similar_user_ratings[movie_id] for movie_id in common_movies_user]\n",
        "\n",
        "        # Calculate the Pearson correlation between the random user and the similar user\n",
        "        correlation_user = pearsonr(user_ratings_common, similar_user_ratings_common)[0]\n",
        "\n",
        "        # Check if the similarity is within the specified threshold interval\n",
        "        if user_based_threshold_interval[0] <= correlation_user <= user_based_threshold_interval[1]:\n",
        "            # Iterate over the similar user's ratings\n",
        "            for movie_id, rating in similar_user_ratings.items():\n",
        "                # Add the rating to the aggregated ratings\n",
        "                aggregated_ratings_user[movie_id] += rating\n",
        "\n",
        "            similar_users_count += 1\n",
        "\n",
        "# Calculate the average ratings from similar users for user-based collaborative filtering\n",
        "average_user_ratings = {movie_id: rating / similar_users_count for movie_id, rating in aggregated_ratings_user.items()}\n",
        "\n",
        "# Sort the average user ratings in descending order\n",
        "sorted_user_ratings = sorted(average_user_ratings.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top-N recommended items for user-based collaborative filtering\n",
        "user_based_recommendations = [item_id for item_id, _ in sorted_user_ratings[:num_recommendations]]\n",
        "\n",
        "# Create the KNNBasic model with item-based collaborative filtering\n",
        "item_based_model = KNNBasic(sim_options={'user_based': False})\n",
        "\n",
        "# Train the item-based model on the full training set\n",
        "item_based_model.fit(trainset)\n",
        "\n",
        "# Get the inner item ID for the items rated by the random user\n",
        "inner_item_ids = [trainset.to_inner_iid(movie_id) for movie_id in user_ratings.keys()]\n",
        "\n",
        "# Use the trained item-based model to get the similarities between the items rated by the random user and other items\n",
        "item_similarities = item_based_model.compute_similarities()\n",
        "\n",
        "# Create a defaultdict to store the aggregated ratings from similar items\n",
        "aggregated_ratings_item = defaultdict(float)\n",
        "similar_items_count = 0\n",
        "\n",
        "# Iterate over the item similarities for item-based collaborative filtering\n",
        "for inner_item_id, similarity_row in enumerate(item_similarities):\n",
        "    # Convert from inner item ID to raw item ID\n",
        "    raw_item_id = trainset.to_raw_iid(inner_item_id)\n",
        "\n",
        "    # Check if the item is rated by the random user\n",
        "    if raw_item_id in user_ratings:\n",
        "        # Get the similarity values for the items rated by the random user\n",
        "        item_similarities_ratings = similarity_row[inner_item_ids]\n",
        "\n",
        "        # Check if any items are sufficiently similar based on the threshold\n",
        "        if any(item_based_threshold_interval[0] <= sim <= item_based_threshold_interval[1] for sim in item_similarities_ratings):\n",
        "            # Iterate over the similar items' ratings\n",
        "            for movie_id, rating in ratings_df_600[ratings_df_600['movie_id'] == raw_item_id][['movie_id', 'rating']].values:\n",
        "                # Add the rating to the aggregated ratings\n",
        "                aggregated_ratings_item[movie_id] += rating\n",
        "\n",
        "            similar_items_count += 1\n",
        "\n",
        "# Calculate the average ratings from similar items for item-based collaborative filtering\n",
        "average_item_ratings = {movie_id: rating / similar_items_count for movie_id, rating in aggregated_ratings_item.items()}\n",
        "\n",
        "# Sort the average item ratings in descending order\n",
        "sorted_item_ratings = sorted(average_item_ratings.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top-N recommended items for item-based collaborative filtering\n",
        "item_based_recommendations = [item_id for item_id, _ in sorted_item_ratings[:num_recommendations]]\n",
        "\n",
        "# Print the recommended items for user-based collaborative filtering\n",
        "print(f\"User-Based Recommendations for user {random_user}:\")\n",
        "for item_id in user_based_recommendations:\n",
        "    print(\"movie id :\", item_id)\n",
        "\n",
        "# Print the recommended items for item-based collaborative filtering\n",
        "print(f\"Item-Based Recommendations for user {random_user}:\")\n",
        "for item_id in item_based_recommendations:\n",
        "    print(\"movie id :\" , item_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhX8MoBjBsbL",
        "outputId": "747e322e-256e-4806-916e-aef10c918144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:4781: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "User-Based Recommendations for user 53:\n",
            "movie id : 2858\n",
            "movie id : 858\n",
            "movie id : 260\n",
            "movie id : 2997\n",
            "movie id : 2396\n",
            "Item-Based Recommendations for user 53:\n",
            "movie id : 480\n",
            "movie id : 318\n",
            "movie id : 2997\n",
            "movie id : 457\n",
            "movie id : 1036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8**"
      ],
      "metadata": {
        "id": "TaPgZS8HEG_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of items to recommend\n",
        "num_recommendations = 5\n",
        "\n",
        "# Randomly select a user\n",
        "random_user = random.choice(ratings_df_600['user_id'].unique())\n",
        "\n",
        "# Create a Reader object to parse the ratings DataFrame\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "\n",
        "# Load the ratings DataFrame into a Surprise Dataset\n",
        "data = Dataset.load_from_df(ratings_df_600[['user_id', 'movie_id', 'rating']], reader)\n",
        "\n",
        "# Build the full trainset\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "# User-Based Collaborative Filtering\n",
        "start_time_user_based = time.time()\n",
        "\n",
        "# Create the KNNBasic model with user-based collaborative filtering\n",
        "user_based_model = KNNBasic(k=30, sim_options={'user_based': True})\n",
        "\n",
        "# Train the user-based model on the full training set\n",
        "user_based_model.fit(trainset)\n",
        "\n",
        "# Get the inner user ID for the random user\n",
        "inner_user_id = trainset.to_inner_uid(random_user)\n",
        "\n",
        "# Use the trained user-based model to get recommendations\n",
        "user_based_recommendations = user_based_model.get_neighbors(inner_user_id, k=num_recommendations)\n",
        "\n",
        "end_time_user_based = time.time()\n",
        "user_based_time = end_time_user_based - start_time_user_based\n",
        "\n",
        "# Item-Based Collaborative Filtering\n",
        "start_time_item_based = time.time()\n",
        "\n",
        "# Create the KNNBasic model with item-based collaborative filtering\n",
        "item_based_model = KNNBasic(k=30, sim_options={'user_based': False})\n",
        "\n",
        "# Train the item-based model on the full training set\n",
        "item_based_model.fit(trainset)\n",
        "\n",
        "# Use the trained item-based model to get item similarities\n",
        "item_similarities = item_based_model.compute_similarities()\n",
        "\n",
        "# Get the top-N items similar to those rated by the random user\n",
        "item_based_recommendations = []\n",
        "\n",
        "for inner_item_id in inner_item_ids:\n",
        "    # Find the most similar items\n",
        "    item_similarities_ratings = item_similarities[inner_item_id]\n",
        "    similar_items = sorted(enumerate(item_similarities_ratings), key=lambda x: x[1], reverse=True)[:num_recommendations]\n",
        "\n",
        "    # Extract the item IDs\n",
        "    similar_item_ids = [trainset.to_raw_iid(item_id) for item_id, _ in similar_items]\n",
        "\n",
        "    item_based_recommendations.extend(similar_item_ids)\n",
        "\n",
        "    # Limit the recommendations to 5 for each technique\n",
        "    item_based_recommendations = item_based_recommendations[:num_recommendations]\n",
        "\n",
        "end_time_item_based = time.time()\n",
        "item_based_time = end_time_item_based - start_time_item_based\n",
        "\n",
        "# Print the recommended items for user-based collaborative filtering\n",
        "print(f\"User-Based Recommendations for user {random_user}: {user_based_recommendations}\")\n",
        "print(f\"Time taken for user-based collaborative filtering: {user_based_time:.4f} seconds\\n\")\n",
        "\n",
        "# Print the recommended items for item-based collaborative filtering\n",
        "print(f\"Item-Based Recommendations for user {random_user}: {item_based_recommendations}\")\n",
        "print(f\"Time taken for item-based collaborative filtering: {item_based_time:.4f} seconds\")\n",
        "\n",
        "# Compare timestamps\n",
        "if user_based_time < item_based_time:\n",
        "    print(\"User-Based Collaborative Filtering is faster.\")\n",
        "elif user_based_time > item_based_time:\n",
        "    print(\"Item-Based Collaborative Filtering is faster.\")\n",
        "else:\n",
        "    print(\"Both methods took the same amount of time.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzHwYA8tEKz2",
        "outputId": "90d30959-eeab-480d-c013-775a529de876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Computing the msd similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "User-Based Recommendations for user 874: [2, 11, 12, 40, 48]\n",
            "Time taken for user-based collaborative filtering: 19.7974 seconds\n",
            "\n",
            "Item-Based Recommendations for user 874: [2987, 3056, 3640, 3154, 2679]\n",
            "Time taken for item-based collaborative filtering: 18.9686 seconds\n",
            "Item-Based Collaborative Filtering is faster.\n"
          ]
        }
      ]
    }
  ]
}
